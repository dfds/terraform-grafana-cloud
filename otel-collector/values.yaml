# Default values for opentelemetry-collector.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
nameOverride: ""
fullnameOverride: ${name}

# Valid values are "daemonset", "deployment", and "statefulset".
mode: "deployment"

# Specify which namespace should be used to deploy the resources into
namespaceOverride: ""
image:
  repository: otel/opentelemetry-collector-k8s

command:
  name: otelcol-k8s

# Base collector configuration.
# Supports templating. To escape existing instances of {{ }}, use {{` <original content> `}}.
# For example, {{ REDACTED_EMAIL }} becomes {{` {{ REDACTED_EMAIL }} `}}.
config:
  exporters:
    otlphttp:
      endpoint: ${otlphttp_endpoint}
      headers:
        Authorization: Basic ${stack_token}
%{ if diagram_otel_receiver_endpoint != "" ~}
    otlp/diagram:
      endpoint: ${diagram_otel_receiver_endpoint}
      tls:
        insecure: true 
%{ endif ~}
    debug: null
  processors:
    batch:
      send_batch_size: 500
      send_batch_max_size: 1000
    filter:
      error_mode: ignore
      logs:
        include:
          severity_number:
            min: "INFO"
            match_undefined: true
%{ if enable_collector_tailsampling ~}
    tail_sampling:
      # https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/tailsamplingprocessor
      policies:
        [
          {
            # use probabilistic sampling services using tailsampling
            name: randomized-policy,
            type: probabilistic,
            probabilistic: { sampling_percentage: ${collector_tailsampling_probablistic_percentage} },
          },
          # always sample if there is an error for services using tailsampling
          {
            name: trace-status-policy,
            type: status_code,
            status_code: { status_codes: [ERROR] },
          },
          {
            # always sample if the force_sample attribute is set to true
            name: force-sample,
            type: boolean_attribute,
            boolean_attribute: { key: app.force_sample, value: true },
          },
        ]
%{ endif ~}
    # If set to null, will be overridden with values based on k8s resource limits
    # memory_limiter: null
%{ if enable_collector_for_external_access ~}
  extensions:
    bearertokenauth:
      token: ${collector_token}
%{ endif ~}
  receivers:
    jaeger: null
    prometheus: null
    zipkin: null
%{ if enable_collector_for_external_access ~}
    otlp:
      protocols:
        grpc:
          endpoint: $${env:MY_POD_IP}:4317
        http:
          endpoint: $${env:MY_POD_IP}:4318
          auth:
            authenticator: bearertokenauth
%{ endif ~}
  service:
%{ if enable_collector_for_external_access ~}
    extensions: [bearertokenauth, health_check]
%{ endif ~}
    pipelines:
      logs:
        exporters:
          - otlphttp
        processors:
          # - memory_limiter
          - filter
          - batch
        receivers:
          - otlp
      metrics:
        exporters:
          # - debug
          - otlphttp
        processors:
          # - memory_limiter
          - batch
        receivers:
          - otlp
      traces:
        exporters:
          # - debug
          - otlphttp
        processors:
          # - memory_limiter
          - batch
%{ if enable_collector_tailsampling ~}
          - tail_sampling
%{ endif ~}
        receivers:
          - otlp
%{ if diagram_otel_receiver_endpoint != "" ~}
      traces/diagram:
        exporters:
          # - debug
          - otlp/diagram
        processors:
          # - memory_limiter
          - batch
        receivers:
          - otlp
%{ endif ~}

# Configuration for ports
# nodePort is also allowed
ports:
  jaeger-compact:
    enabled: false
  jaeger-thrift:
    enabled: false
  jaeger-grpc:
    enabled: false
  zipkin:
    enabled: false
%{ if enable_collector_for_external_access ~}
  external:
    enabled: true
    containerPort: 4318
    servicePort: 80
    hostPort: 4318
    protocol: TCP
%{ endif ~}
# Resource limits & requests. Update according to your own use case as these values might be too low for a typical deployment.
resources:
  limits:
    cpu: 1
    memory: 512Mi
  requests:
    cpu: 20m
    memory: 512Mi
podAnnotations: {}

podLabels: { owner: "${owner}", stack: "${stack}" }

# Common labels to add to all otel-collector resources. Evaluated as a template.
additionalLabels: { owner: "${owner}", stack: "${stack}" }
#  app.kubernetes.io/part-of: my-app

annotations: {}

# liveness probe configuration
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
##
livenessProbe:
  # Number of seconds after the container has started before startup, liveness or readiness probes are initiated.
  # initialDelaySeconds: 1
  # How often in seconds to perform the probe.
  # periodSeconds: 10
  # Number of seconds after which the probe times out.
  # timeoutSeconds: 1
  # Minimum consecutive failures for the probe to be considered failed after having succeeded.
  # failureThreshold: 1
  # Duration in seconds the pod needs to terminate gracefully upon probe failure.
  # terminationGracePeriodSeconds: 10
  httpGet:
    port: 13133
    path: /

# readiness probe configuration
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
##
readinessProbe:
  # Number of seconds after the container has started before startup, liveness or readiness probes are initiated.
  # initialDelaySeconds: 1
  # How often (in seconds) to perform the probe.
  # periodSeconds: 10
  # Number of seconds after which the probe times out.
  # timeoutSeconds: 1
  # Minimum consecutive successes for the probe to be considered successful after having failed.
  # successThreshold: 1
  # Minimum consecutive failures for the probe to be considered failed after having succeeded.
  # failureThreshold: 1
  httpGet:
    port: 13133
    path: /
